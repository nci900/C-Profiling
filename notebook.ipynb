{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling and Optimising C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# The jupyter notebook is launched from your $HOME directory.\n",
    "# Change the working directory to the C-Debugging directory\n",
    "# which was created in your username directory under /scratch/vp91\n",
    "os.chdir(os.path.expandvars(\"/scratch/vp91/$USER/C-Profiling/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Appending to a `List`\n",
    "[test_append.c](./test_append.c) has some simple code to build a large list of square numbers.\n",
    "\n",
    "We can compile it with optimisations turned on below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc test_append.c list.c -o test_append -g -Wall -Wextra -Wpedantic -O3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest profiling tool we can use is `time`. This just tells us how long the program took to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!time ./test_append"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`time` can be useful as a quick check, but timing can vary between individual runs, and a proper profiling tool will give us more useful information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 `gprof`\n",
    "One widely available option is `gprof`. `gprof` tracks how many times each function was called, and periodically samples the program counter to estimate the time spent in each function.\n",
    "To use it, we need to compile with the `-pg` flag. We also disable function reordering and inlining to make sure we can see the callgraph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc test_append.c list.c -o pg_append -g -pg -Wall -Wextra -Wpedantic -O3 -fno-reorder-functions -fno-inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adds extra profiling code to our binary. After compiling, run the application as normal to generate a `gmon.out` file, and then view the profile information with the `gprof` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!./pg_append && gprof ./pg_append"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since timing is based on periodic sampling it can vary significantly between runs. Accuracy can be improved by running multiple times and combining outputs with the `-s` flag.\n",
    "\n",
    "For example, the block below runs `pg_append` ten times to create ten different profile output files. We set the `GMON_OUT_PREFIX` environment variable so that each output file has a unique name (of the format `gmon.out.<pid>` where `<pid>` is the process ID)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm gmon.out.*; for i in {1..10}; do GMON_OUT_PREFIX=\"gmon.out\" ./pg_append; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can combine the profiles into a single summary file (`gmon.sum`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gprof -s ./pg_append gmon.out.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then analyse the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!gprof ./pg_append gmon.sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The profile makes it clear that most of the time is spent in the `list_append` function, but it's not so clear what in particular is slow about it.\n",
    "\n",
    "We can get extra information using the `callgrind` tool of `valgrind`.\n",
    "\n",
    "### 1.2 `callgrind`\n",
    "The `callgrind` tool will count the number of instructions executed for each line of the program, giving reproducible profiling information. This can be very useful, but will significantly increase execution time. The instruction count also does not directly correspond to execution time, since for example an I/O instruction could take significantly longer under memory pressure, so `callgrind` is best used in combination with timing tools (like `gprof`).\n",
    "\n",
    "In this case, we don't want the `-pg` compiler flag, since that will add overhead that we don't need, but we do still need the `-g` flag to include debug symbols (needed to link instructions to lines in the source code). Note that with optimisations enabled, some lines might not have any associated instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc test_append.c list.c -o test_append -g -Wall -Wextra -Wpedantic -O3 -fno-reorder-functions -fno-inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run `callgrind` with the block below (this will take a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm callgrind.out.*; valgrind --tool=callgrind ./test_append"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates a `callgrind.out.<pid>` file which we can analyse with `callgrind_annotate`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!callgrind_annotate callgrind.out.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us that the vast majority of instructions are happening inside the call to `realloc`. We already know that `list_append` is the bottleneck, and now we know that most of the work in that function is done in `realloc`, so we can use this information to decide on an optimisation strategy.\n",
    "\n",
    "Try improving the code so that building the list of squared numbers is faster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### HINTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Allocating memory is generally slow. `realloc` has some optimisations that will sometimes make it faster, but it's not guaranteed.\n",
    "* Our program knows ahead of time that there will be exactly `N` items in the list, but it currently calls `realloc` every time a new item is appended.\n",
    "* By changing how `List` manages its memory, we can reduce the number of calls to `realloc` to avoid unnecessary work.\n",
    "\n",
    "If you just want to see the solution, you can look at [test_append_optimised.c](./test_append_optimised.c), [list_optimised.c](./list_optimised.c) and [list_optimised.h](./list_optimised.h)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix-matrix multiplication gives us a chance to examine some other optimisation techniques.\n",
    "Take a look at [matmul.c](./matmul.c). It repeats a 500 x 500 matrix multiplication 100 times to test the `matmul` function.\n",
    "\n",
    "It can be compiled with the command below. From a VDI session, try profiling it with Arm MAP and using the profile information to speed it up. For larger applications, you may need some extra compile flags to get the most information. See [the documentation](https://developer.arm.com/documentation/101136/2100/MAP/Get-started-with-MAP/Prepare-a-program-for-profiling?lang=en) for details.\n",
    "\n",
    "You may also find [c.godbolt.org](https://c.godbolt.org) handy for fine-grained analysis of the instructions output by the compiler. With some practice, this can be useful for checking things like auto-vectorisation. Don't forget to set the `-O3` flag, as well as `-mavx2` to tell the compiler that AVX2 vector instructions are available (or `-mavx512` for 512 bit vector instructions, or specify the exact CPU architecture like `-march=sapphirerapids`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcc matmul.c -o matmul -g -Wall -Wextra -Wpedantic -lm -O3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### HINTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of inefficiencies in the code related to:\n",
    "* Memory layout\n",
    "* Cache friendliness\n",
    "* Compiler friendliness (mainly for auto-vectorisation)\n",
    "\n",
    "Consider optimisations such as:\n",
    "* Avoiding pointer chasing (it's faster to just follow one pointer than to follow one and then annother)\n",
    "* Using contiguous memory (it's generally faster to work with a single large block of memory instead of many smaller blocks)\n",
    "* Tweaking loop ordering (accessing elements in the same order they're stored in memory can give better cache performance and help the compiler to auto-vectorise)\n",
    "* Using the `restrict` keyword to promise to the compiler that the `A`, `B` and `C` matrices don't overlap in memory (the compiler won't assume this by default, which can hinder some optimisations like auto-vectorisation).\n",
    "  - `restrict` can be used with standard pointers as `double *restrict my_ptr;` (it applies to the left like `const`) and with arrays as `double array[restrict N]`\n",
    " \n",
    "See [matmul_optimised.c](./matmul_optimised.c) for an example solution. Note that further optimisation is possible, and libraries such as BLAS and BLIS are significantly faster."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
